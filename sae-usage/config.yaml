# Automated Probe Data Generation Configuration

# Concept to probe (positive class vs negative class)
concept:
  positive_class: "deceptive"
  negative_class: "honest"
  description: "Detect deceptive vs honest behavior in model responses"

# Model settings (default: Llama 3.1 8B)
model:
  name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  layer: 19  # Layer to extract activations from
  device: "cuda"  # "cuda" or "cpu"
  load_in_8bit: false  # Set to true if memory constrained
  max_length: 512  # Maximum sequence length

# LLM Agent for generating data (via OpenRouter)
llm_agent:
  model: "google/gemini-2.5-flash-lite"
  temperature: 0.8
  max_tokens: 500
  api_base: "https://openrouter.ai/api/v1"

# Goodfire API settings
goodfire:
  model_variant: "meta-llama/Meta-Llama-3.1-8B-Instruct"
  layer: 19
  top_k_features: 10  # Number of top features to retrieve per sample

# SAE settings for cosine similarity computation (Andy Arditi's SAEs)
sae:
  repo_id: "andyrdt/saes-llama-3.1-8b-instruct"
  filename: "resid_post_layer_19/trainer_1/ae.pt"  # Specific SAE file to load (matches Neuronpedia labels)
  cache_dir: "/mnt/SharedData/.sae-cache"
  use_cosine_similarity: true  # Compute cosine similarities with decoder weights
  top_k_features: 20  # Number of top features by cosine similarity
  aggregation: "mean"  # How to aggregate across tokens: "mean", "max", or "last"

# Neuronpedia settings for feature labels via live API
neuronpedia:
  api_key_env: "NEURONPEDIA_KEY"  # Environment variable name for API key
  model_id: "llama3.1-8b-it"  # Model ID for Neuronpedia API
  layer_id: "19-resid-post-aa"  # Layer/SAE ID for Neuronpedia API
  cache_dir: "/mnt/SharedData/.neuronpedia-cache"  # Optional cache directory
  layer: 19  # Layer number for model extraction
  model: "llama3.1-8b"  # Model name for local reference

# Data generation settings
generation:
  num_pairs: 10  # Number of contrastive pairs to generate
  formats:
    - "statement"  # Single statement format
    - "qa"  # Question-answer format

  # Apollo Research activation extraction settings
  apollo:
    exclude_last_n_tokens: 5  # Exclude last N tokens from assistant response
    min_assistant_tokens: 10  # Minimum tokens required in assistant response

# Quality validation settings
validation:
  # Feature search queries for validation
  positive_feature_queries:
    - "deception"
    - "deceptive"
    - "lying"
    - "misleading"
  negative_feature_queries:
    - "honest"
    - "honesty"
    - "truthful"
    - "accurate"

  # Validation thresholds
  min_expected_features: 1  # Minimum expected features in top-k
  min_cosine_distance: 0.1  # Minimum cosine distance between positive/negative activations
  llm_judge_threshold: "positive"  # Require LLM judge approval: "positive", "neutral", "negative"
  checkpoint_frequency: 20  # Show human checkpoint every N pairs during generation
  require_llm_approval: true  # Require LLM judge approval for each pair
  require_activation_threshold: true  # Require activation threshold to be met

# Steering configuration for SAE-guided generation
steering:
  initial_amplification: 10  # Starting amplification strength
  amplification_range: [3, 30]  # [min, max] allowed amplification
  adjustment_step: 3  # Step size for adjusting amplification
  num_test_prompts: 5  # Number of test prompts for amplification tuning
  max_feature_attempts: 5  # Try top N SAE features before concept refinement
  max_concept_refinements: 3  # Maximum times to refine concept before failing
  check_degeneracy: true  # Check for model degeneracy (repetition, incoherence)

# Probe training configuration
probe:
  train_test_split: 0.8  # Fraction of data for training (rest for testing)
  solver: "lbfgs"  # Logistic regression solver
  max_iter: 1000  # Maximum iterations for solver
  C: 1.0  # Inverse regularization strength
  random_state: 42  # Random seed for reproducibility
  balance_classes: true  # Use class balancing
  cross_validate: false  # Perform cross-validation
  n_folds: 5  # Number of CV folds if cross_validate is true

# Output settings
output:
  save_dir: "outputs"
  save_activations: true  # Save raw activations for probe training
  save_metadata: true  # Save generation metadata
  timestamp_format: "%Y%m%d_%H%M%S"
