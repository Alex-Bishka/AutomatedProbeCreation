{
  "method": "lr",
  "model_name": "llama-8b",
  "train_data": "repe_honesty__plain",
  "train_on_policy": false,
  "detect_layers": [
    0,
    1,
    2,
    3,
    4,
    5,
    6,
    7,
    8,
    9,
    10,
    11,
    12,
    13,
    14,
    15,
    16,
    17,
    18,
    19,
    20,
    21,
    22,
    23,
    24,
    25,
    26,
    27,
    28,
    29,
    30,
    31
  ],
  "use_local_sae_acts": false,
  "use_goodfire_sae_acts": false,
  "sae_latent_whitelist": null,
  "detect_num_latents": null,
  "pw_locked": false,
  "lora_path": null,
  "reg_coeff": 10.0,
  "normalize_acts": true
}